{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analisis de sentimientos de comentarios de películas**\n",
    "\n",
    "### Integrantes:\n",
    "* Ángela Vargas\n",
    "* Juan Martin Santos\n",
    "* Daniel Osorio\n",
    "\n",
    "## **Caso**\n",
    "Análisis de sentimientos de películas. Este proyecto tiene comentarios de películas\n",
    "en español, que deben ser clasificadas en las categorías de positivo, negativo. Esto por medio de técnicas para el procesamiento de lenguaje natural.\n",
    "\n",
    "## **Entendimiento de los datos**\n",
    "Primero vamos a cargar los datos en un DataFrame para luego revisar la calidad de estos en cuanto a completitud, consistencia y unicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# Se descargan las stopwords (despues se usan para eliminar palabras que no aportan informacion)\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "# Este comando es requerido para que las visualizaciones se muestren en este notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_rows', 50) # Numero máximo de filas a mostar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos en formato CSV\n",
    "# Los datos son almacenados en memoria usando una estructura de datos de Pandas conocida como dataframe\n",
    "df_reviews = pd.read_csv('./data/MovieReviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el numero de filas y columnas del dataframe\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>3548</td>\n",
       "      <td>No hace falta decir que se debe hacer un Módic...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>2087</td>\n",
       "      <td>\"Ven a deshacerme\" parece obtener muchas opini...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>4010</td>\n",
       "      <td>Directamente al video y con buena razón.Es com...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>698</td>\n",
       "      <td>Esta es quizás la serie de televisión con el m...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>646</td>\n",
       "      <td>La película es buena y creo que Tiffany Amber ...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                          review_es  \\\n",
       "3548        3548  No hace falta decir que se debe hacer un Módic...   \n",
       "2087        2087  \"Ven a deshacerme\" parece obtener muchas opini...   \n",
       "4010        4010  Directamente al video y con buena razón.Es com...   \n",
       "698          698  Esta es quizás la serie de televisión con el m...   \n",
       "646          646  La película es buena y creo que Tiffany Amber ...   \n",
       "\n",
       "     sentimiento  \n",
       "3548    negativo  \n",
       "2087    positivo  \n",
       "4010    negativo  \n",
       "698     positivo  \n",
       "646     positivo  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra de 5 filas aleatorias del dataframe\n",
    "df_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "review_es      object\n",
       "sentimiento    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el tipo de datos de cada columna\n",
    "df_reviews.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positivo    2500\n",
       "negativo    2500\n",
       "Name: sentimiento, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra los diferentes valores de la columna sentimiento\n",
    "df_reviews['sentimiento'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, tenemos 5000 registros (reviews) de los cuales la mitad corresponden a reviews positivas y la otra mitad a reviews negativas. Existe consistencia en los valores de la columna sentimiento. Como existen 2500 positivos y 2500 negativos, podemos decir que la completitud de los datos para la columna sentimiento es del 100%, vamos a revisar ahora la completitud para la columna *review_es*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra la cantidad de valores nulos para la columna review_es\n",
    "df_reviews['review_es'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, tenemos una completitud del 100% en la columna review_es, por lo que no es necesario realizar ningún tipo de limpieza de datos.\n",
    "## **Preprocesamiento**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como libreria para el procesamiento de lenguaje natural vamos a utilizar NLTK (Natural Language Tool Kit). Primero definimos un set de stopwords en español. Las stopwords son esas palabras que no aportan significado al texto para la tarea de clasificación. Estas palabras vacías pueden ser conectores propios del idioma, pronombres o puntos y comas. Algunas stopwords del español son por ejemplo las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tuvieras', 'tendríais', 'tiene', 'eras', 'tuve', 'habré', 'tuvo', 'sentido', 'ya', 'por', 'tuviese', 'vuestro', 'estoy', 'fueras', 'sintiendo', 'hayas', 'ni', 'tengan', 'estuviésemos', 'sentidos', 'estuviste', 'estaría', 'estado', 'poco', 'tendríamos', 'nuestros', 'tendrían', 'estábamos', 'éramos', 'hayamos', 'tendrás', 'vuestros', 'porque', 'estuvisteis', 'mías', 'un', 'fueses', 'ante', 'tuyo', 'otros', 'suyas', 'hubieseis', 'estamos', 'estuvieseis', 'hubiéramos', 'tengo', 'mucho', 'siente', 'tengáis', 'seré', 'esta', 'fue', 'habiendo', 'habías', 'desde', 'hubiese', 'suyos', 'nada', 'será', 'fuéramos', 'tendréis', 'estemos', 'tened', 'contra', 'estaremos', 'estad', 'teníamos', 'muy', 'hubiste', 'seremos', 'teníais', 'uno', 'estuvieron', 'mi', 'han', 'estos', 'tendrías', 'estabas', 'unos', 'fuisteis', 'habrán', 'estando', 'cuando', 'hay', 'sí', 'una', 'míos', 'fuiste', 'ellos', 'vosotros', 'estaba', 'tenido', 'nuestras', 'tuvierais', 'tuyos', 'estuviese', 'esos', 'fueseis', 'sería', 'tú', 'os', 'suya', 'habremos', 'habría', 'tuvisteis', 'habían', 'algunas', 'tenidos', 'te', 'seamos', 'estas', 'haya', 'estar', 'seáis', 'he', 'o', 'entre', 'antes', 'sea', 'sentidas', 'tenías', 'tuvieran', 'tendría', 'tenéis', 'durante', 'tendrá', 'habríamos', 'hube', 'son', 'tengamos', 'fueran', 'tienen', 'tuvieses', 'habrá', 'fuesen', 'hubo', 'seríais', 'habríais', 'mío', 'sus', 'tendré', 'me', 'lo', 'hayáis', 'hubimos', 'qué', 'está', 'eso', 'estuvo', 'estéis', 'estarías', 'hubieses', 'sean', 'donde', 'habidos', 'en', 'estuvieses', 'habidas', 'tendrán', 'estás', 'habida', 'sentid', 'soy', 'ti', 'estuve', 'es', 'nuestro', 'les', 'habrás', 'fuerais', 'de', 'estarán', 'algunos', 'vosotras', 'con', 'todos', 'tenemos', 'cual', 'tienes', 'ese', 'las', 'serás', 'nuestra', 'estará', 'tendremos', 'fui', 'otra', 'tengas', 'no', 'él', 'los', 'tuviera', 'nos', 'fuésemos', 'habrías', 'tuya', 'estabais', 'hubisteis', 'estuviéramos', 'más', 'mí', 'había', 'estén', 'mía', 'ellas', 'tuvieseis', 'estuviesen', 'tu', 'eres', 'hubiésemos', 'habrían', 'tenían', 'mis', 'suyo', 'habéis', 'habido', 'teniendo', 'al', 'estarás', 'hayan', 'fuese', 'estarían', 'el', 'otro', 'estadas', 'como', 'somos', 'hasta', 'algo', 'tuviésemos', 'pero', 'hubiesen', 'hubiera', 'habíais', 'este', 'sois', 'tuviéramos', 'tus', 'esté', 'para', 'serías', 'ella', 'habíamos', 'estaríais', 'estuvieran', 'fuera', 'nosotros', 'serán', 'has', 'yo', 'muchos', 'seas', 'seríamos', 'sin', 'hubierais', 'fuimos', 'estés', 'tuyas', 'tuviesen', 'estaré', 'hubieron', 'e', 'estaban', 'vuestras', 'erais', 'tenga', 'todo', 'estaríamos', 'hubieran', 'era', 'sentida', 'estuvieras', 'seréis', 'estáis', 'sobre', 'esa', 'que', 'quienes', 'tuvimos', 'tuvieron', 'estados', 'fueron', 'la', 'nosotras', 'estuvimos', 'vuestra', 'esto', 'a', 'también', 'otras', 'hubieras', 'serían', 'estada', 'están', 'estaréis', 'eran', 'estuvierais', 'tenía', 'y', 'tuviste', 'su', 'ha', 'tenidas', 'quien', 'del', 'esas', 'habréis', 'hemos', 'estuviera', 'se', 'le', 'tenida', 'tanto'}\n"
     ]
    }
   ],
   "source": [
    "# Seteamos las stopwords en español\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "# Agregamos a las stopwords tambien los signos de puntuacion\n",
    "# Se agrega una u a la izquierda para indicar que es un string unicode, sin embargo no es necesario\n",
    "stop_words.extend([u'.', u'[', ']', u',', u';', u'', u')', u'),', u' ', u'('])\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se utiliza el Snowball stemming algorithm, este algoritmo lo que hace es transformar/reducir las palabras a su forma base, aqui hacemos un ejemplo con la palabra 'corriendo' y 'correr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('spanish')\n",
    "snowball_stemmer.stem('corriendo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('correr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tokenizar utilizamos la función *wordpunct_tokenize()* que tiene la libreria NLTK. Esta función, a diferencia de la función *word_tokenize()*, almacena los signos de puntuación como tokens diferentes. El *word.isapha()* es para revisar que la palabra no sea de una sola letra como por ejemplo 'y'. Luego se revisa que la palabra en minuscula no esté entre las stopwords antes de tokenizarla. A continuación un ejemplo de como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ejemplo', 'reseña', 'español']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de como se tokenizan las palabras de una reseña\n",
    "review_example = \"Este es un ejemplo de una reseña en español.\"\n",
    "review_tokens = [word.lower() for word in wordpunct_tokenize(review_example) if word.isalpha() and word.lower() not in stop_words]\n",
    "print(review_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicamos esto para cada fila de la columna 'review_es'. Para esto definimos la funcion lamba a cada fila de la columna review utilizando el metodo apply. Se hace todo el proceso de tokenizacion mostrado anteriormente y se vuelve a juntar el texto con el metodo join(). El string resultante es ahora el nuevo valor para la fila en la columna 'review_es'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_es'] = df_reviews['review_es'].apply(lambda x: ' '.join([snowball_stemmer.stem(word.lower()) for word in wordpunct_tokenize(x) if (word.isalpha() and word.lower() not in stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    si busc pelicul guerr tipic asi not aficion gu...\n",
       "1    supong director pelicul luj sent busc abrig gr...\n",
       "2    dificil cont pelicul estrop disfrut esper vien...\n",
       "3    pelicul comienz lent estil vid wallac napalm a...\n",
       "4    pelicul verdader accion maxim expresion mejor ...\n",
       "Name: review_es, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews['review_es'].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el *CountVectorizer()* que nos da la cuenta de cada de cada palabra para cada review, lo cual podemos usar para los modelos que crearemos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la cuenta de cada palabra para cada review\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_reviews['review_es'])\n",
    "y = df_reviews['sentimiento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos**\n",
    "\n",
    "### **Multinomial Naive Bayes**\n",
    "\n",
    "Escogimos este algoritmo porque trabaja muy bien con datos discretos, lo cual lo hace especialmente bueno para una tarea de clasificacion como si el comentario es positivo o negativo. El algoritmo realiza el calculo de la probabilidad de que cada palabra de una review pertenezca a una clase (positivo o negativo) y luego utiliza estas probabilidades con el teorema de Bayes para calcular la probabilidad de que una review pertenezca a una clase. El algoritmo asume que la probabilidad de una palabra es independiente de las otras, por lo cual es llamado 'naive' Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud sobre entrenamiento: 0.95\n",
      "Exactitud sobre test: 0.82\n",
      "Recall: 0.7650727650727651\n",
      "Precisión: 0.8382687927107062\n",
      "Puntuación F1: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "multinomialNB = MultinomialNB()\n",
    "multinomialNB.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_train_pred = multinomialNB.predict(X_train)\n",
    "y_pred = multinomialNB.predict(X_test)\n",
    "\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(y_train, y_train_pred))\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred, pos_label='positivo')))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred, pos_label='positivo')))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred, pos_label='positivo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[448,  71],\n",
       "       [113, 368]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de confusion\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

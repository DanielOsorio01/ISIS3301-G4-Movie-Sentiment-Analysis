{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analisis de sentimientos de comentarios de películas**\n",
    "\n",
    "### Integrantes:\n",
    "* **Ángela Vargas:** Desempeñó el rol de Líder de analítica al encargarse de probar los modelos y determinar cual es el mejor según las restricciones del cliente \n",
    "* **Juan Martin Santos:** Desempeñó el rol de lider de negocio asumiendo la responsabilidad de identificar la estrategia utilizada para alcanzar el objetivo del negocio.\n",
    "* **Daniel Osorio:** Desempeño el rol de Líder de proyecto definiendo las fechas de reuniones y asignando los tareas equitativamente entre los integrantes. También desempñó el rol de lider de datos al haber realizado la limpieza de los datos y haberla dejado disponible para todos los integrantes.\n",
    "\n",
    "## **Caso**\n",
    "\n",
    "| Pregunta de entendimiento | Justificación |\n",
    "| --- | --- |\n",
    "| **¿Cuáles son los objetivos del negocio?** | En este caso asumimos que nuestro cliente es una página de streaming que desea tomar las reviews de las películas de su plataforma para y determinar si esta debe reemplazarse por una nueva película que pueda generar un mejor sentimiento a la audiencia o no. El sentimiento viene dado por la cantidad de buenas y malas reviews que tenga, entre mas reviews negativas tenga, peor es el sentimiento de la película. |\n",
    "| **¿Cuáles son los criterios de éxito?** | Nuestro principal criterio de éxito es la precisión, ya que determinar erroneamente el sentimiento de una película puede afectar al negocio en la toma de decisiones sobre el estado de una película dentro de su plataforma. |\n",
    "| **¿Cuál es la Oportunidad / problema del negocio?** | Al usar aprendizaje automático, el negocio se puede beneificar al ser más eficiente en la forma en que determinar si una película debe quedarse o no, haciendo que el público en general esté más satisfecho con el contenido que consume aumentando la fidelidad y posibles ganancias. |\n",
    "| **¿Cuál es el Enfoque Analítico?** | Desde el punto de vista del aprendizaje automático, el problema se enfoca en algoritmos de clasificación en el análisis de textos. De esta forma, se etá realizando el análisis de sentimientos de películas. Este proyecto tiene comentarios de películas en español, que deben ser clasificadas en las categorías de positivo, negativo. Esto por medio de técnicas para el procesamiento de lenguaje natural. |\n",
    "| **¿Quienes estan siendo beneficiados?** | El principal beneficiado a parte del negocio son los clientes que van a poder determinar tener más decisión sobre el catálogo que ofrece la empresa de streaming |\n",
    "| **¿Cuáles son las técnicas y algoritmos a utilizar?** | Haremos uso de Multinomial Naives Bayes, Random forest y regresión logística |\n",
    "\n",
    "## **Entendimiento de los datos**\n",
    "Primero vamos a cargar los datos en un DataFrame para luego revisar la calidad de estos en cuanto a completitud, consistencia y unicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\amvar\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\amvar\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# Se descargan las stopwords (despues se usan para eliminar palabras que no aportan informacion)\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from langdetect import detect\n",
    "from cleanText import cleanText\n",
    "\n",
    "# Este comando es requerido para que las visualizaciones se muestren en este notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_rows', 50) # Numero máximo de filas a mostar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/MovieReviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27584\\1941974075.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Lectura de datos en formato CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Los datos son almacenados en memoria usando una estructura de datos de Pandas conocida como dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/MovieReviews.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/MovieReviews.csv'"
     ]
    }
   ],
   "source": [
    "# Lectura de datos en formato CSV\n",
    "# Los datos son almacenados en memoria usando una estructura de datos de Pandas conocida como dataframe\n",
    "df_reviews = pd.read_csv('./data/MovieReviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el numero de filas y columnas del dataframe\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra de 5 filas aleatorias del dataframe\n",
    "df_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el tipo de datos de cada columna\n",
    "df_reviews.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Muestra los diferentes valores de la columna sentimiento\n",
    "df_reviews['sentimiento'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, tenemos 5000 registros (reviews) de los cuales la mitad corresponden a reviews positivas y la otra mitad a reviews negativas. Existe consistencia en los valores de la columna sentimiento. Como existen 2500 positivos y 2500 negativos, podemos decir que la completitud de los datos para la columna sentimiento es del 100%, vamos a revisar ahora la completitud para la columna *review_es*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la cantidad de valores nulos para la columna review_es\n",
    "df_reviews['review_es'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, verificaremos la unicidad de los valores de la columnas \"review_es\". Este paso es importante ya que al identificar registros duplicados, podemos tomar medidas que nos ayudan a mejorar la precision y eficiencia de los modelos que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews['review_es'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, verificaremos el idioma en el que se encuentran los comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero creamos el diccionario de los idiomas en los que estan los comentarios\n",
    "langs = {}\n",
    "\n",
    "# Luego iteramos sobre cada comentario y determinamos su idioma\n",
    "for index, row in df_reviews.iterrows():\n",
    "    if detect(row['review_es']) not in langs:\n",
    "        langs[detect(row['review_es'])] = 1\n",
    "    else:\n",
    "        langs[detect(row['review_es'])] += 1\n",
    "\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, tenemos una completitud del 100% en la columna review_es, por lo que no es necesario realizar ningún tipo de limpieza de datos a nivel de intregridad y completitud de los datos. Sin embargo, nos percatamos que dentro del conjunto de datos hay comentarios que no estan en español y que hay comentarios repetidos, por lo que obtaremos por eliminar estas filas, esta decision se toma debido a las necesidades del negocio de analisar comentarios en español.\n",
    "## **Preprocesamiento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero comenzaremos por cambiar las variables de sentimiento que son positivo y negativo por 0 (siendo negativo) y 1 (siendo positivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews['sentimiento'].replace({'negativo': 0, 'positivo': 1}, inplace=True)\n",
    "df_reviews['sentimiento'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encargarnos de los comentarios repetidos obtamos por eliminar los registros. Estos no significan una gran parte de los datos por lo que quitarlos nos ayudara en el mejor funcionamiento del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_es'].drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como libreria para limpiar las filas con los comentarios en ingles usaremos langdetect que nos permitira tomar todo el comentario y determinar el idioma en el que esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteramos en cada una de las filas del dataframe\n",
    "for index, row in df_reviews.iterrows():\n",
    "    # Si el valor de la fila es en la elimina\n",
    "    if detect(row['review_es']) != 'es':\n",
    "        df_reviews.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como libreria para el procesamiento de lenguaje natural vamos a utilizar NLTK (Natural Language Tool Kit). Primero definimos un set de stopwords en español. Las stopwords son esas palabras que no aportan significado al texto para la tarea de clasificación. Estas palabras vacías pueden ser conectores propios del idioma, pronombres o puntos y comas. Algunas stopwords del español son por ejemplo las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tengan', 'habido', 'estada', 'estarás', 'esa', 'no', 'sentido', 'estemos', 'estuve', 'tendrían', 'hubierais', 'mis', 'estará', 'yo', 'fueses', 'estuvieran', 'fuesen', 'qué', 'quienes', 'éramos', 'donde', 'hubieses', 'teníais', 'estuvierais', 'tened', 'será', 'eso', 'habrían', 'tuvisteis', 'le', 'pero', 'otro', 'ti', 'estaríais', 'tuvo', 'mío', 'hubisteis', 'tendrás', 'antes', 'soy', 'teníamos', 'sentidos', 'estoy', 'hemos', 'para', 'tuyo', 'a', 'hubieron', 'suyo', 'fuimos', 'estado', 'su', 'también', 'unos', 'esto', 'tenidas', 'tenida', 'hubiste', 'habiendo', 'tuvieses', 'nosotras', 'estuvisteis', 'eres', 'como', 'tengas', 'somos', 'habré', 'tienen', 'tuvieran', 'ya', 'míos', 'cuando', 'hubieras', 'habríais', 'o', 'estás', 'suya', 'estarán', 'estaba', 'tendremos', 'habríamos', 'estuvieron', 'fueras', 'habida', 'es', 'poco', 'has', 'tanto', 'con', 'tenido', 'habría', 'fueran', 'esas', 'estáis', 'tuyos', 'sin', 'fuisteis', 'tendréis', 'porque', 'ese', 'serían', 'algunos', 'tuvieseis', 'contra', 'habrán', 'estaríamos', 'otros', 'esté', 'fueseis', 'todo', 'sí', 'tuviese', 'desde', 'sea', 'sentidas', 'esos', 'mía', 'habrá', 'que', 'nosotros', 'vuestra', 'el', 'sintiendo', 'estéis', 'estabais', 'ha', 'entre', 'mi', 'serías', 'fuéramos', 'siente', 'hay', 'tuvierais', 'tendría', 'todos', 'uno', 'seremos', 'hayan', 'tengamos', 'serás', 'había', 'tu', 'fue', 'seáis', 'las', 'ni', 'tuyas', 'estamos', 'hayamos', 'estaremos', 'se', 'hube', 'habías', 'hubiésemos', 'tuviésemos', 'hubimos', 'habrías', 'tendrán', 'estuviera', 'nuestra', 'suyas', 'sean', 'tuvieron', 'tiene', 'sois', 'tendríais', 'él', 'estén', 'muy', 'les', 'hubieseis', 'estad', 'seríamos', 'hubieran', 'hasta', 'y', 'estando', 'tenidos', 'ella', 'por', 'tuvieras', 'tenemos', 'fuiste', 'estábamos', 'quien', 'eran', 'habíais', 'estuviesen', 'habidos', 'de', 'vuestros', 'fuerais', 'sus', 'estaréis', 'estuvieras', 'hubiese', 'eras', 'tendríamos', 'estar', 'tuya', 'tuviesen', 'está', 'la', 'estuvieseis', 'ante', 'me', 'este', 'hayas', 'vosotras', 'haya', 'habremos', 'algo', 'estaría', 'tú', 'estuvo', 'era', 'fuera', 'tienes', 'tengáis', 'estarían', 'fueron', 'estados', 'teniendo', 'vuestras', 'estuviéramos', 'tendrías', 'tenían', 'tuvimos', 'esta', 'fui', 'durante', 'sería', 'al', 'hubiesen', 'suyos', 'muchos', 'estuvimos', 'mucho', 'tuviéramos', 'habían', 'estos', 'otras', 'fuese', 'nuestro', 'estaré', 'tenéis', 'estas', 'del', 'tuve', 'tenía', 'sentid', 'habéis', 'en', 'estadas', 'están', 'serán', 'cual', 'fuésemos', 'vuestro', 'seamos', 'te', 'estuviésemos', 'mí', 'tuviste', 'más', 'tendrá', 'habrás', 'he', 'estuvieses', 'erais', 'tenías', 'ellas', 'tendré', 'tuviera', 'han', 'hubo', 'estés', 'e', 'estarías', 'lo', 'son', 'seríais', 'vosotros', 'hubiéramos', 'ellos', 'algunas', 'un', 'sobre', 'nuestros', 'estuviese', 'seréis', 'estaban', 'mías', 'estabas', 'tus', 'nos', 'tenga', 'otra', 'sentida', 'hubiera', 'los', 'tengo', 'nada', 'habidas', 'os', 'hayáis', 'nuestras', 'seré', 'estuviste', 'habíamos', 'una', 'habréis', 'seas', '.', '[', ']', ',', ';', '', ')', '),', ' ', '(']\n"
     ]
    }
   ],
   "source": [
    "# Seteamos las stopwords en español\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "# Agregamos a las stopwords tambien los signos de puntuacion\n",
    "# Se agrega una u a la izquierda para indicar que es un string unicode, sin embargo no es necesario\n",
    "stop_words = list(stop_words)\n",
    "stop_words.extend([u'.', u'[', ']', u',', u';', u'', u')', u'),', u' ', u'('])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se utiliza el Snowball stemming algorithm, este algoritmo lo que hace es transformar/reducir las palabras a su forma base, aqui hacemos un ejemplo con la palabra 'corriendo' y 'correr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('spanish')\n",
    "snowball_stemmer.stem('corriendo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer.stem('correr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tokenizar utilizamos la función *wordpunct_tokenize()* que tiene la libreria NLTK. Esta función, a diferencia de la función *word_tokenize()*, almacena los signos de puntuación como tokens diferentes. El *word.isapha()* es para revisar que la palabra no sea de una sola letra como por ejemplo 'y'. Luego se revisa que la palabra en minuscula no esté entre las stopwords antes de tokenizarla. A continuación un ejemplo de como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de como se tokenizan las palabras de una reseña\n",
    "review_example = \"Este es un ejemplo de una reseña en español.\"\n",
    "review_tokens = [word.lower() for word in wordpunct_tokenize(review_example) if word.isalpha() and word.lower() not in stop_words]\n",
    "print(review_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicamos esto para cada fila de la columna 'review_es'. Para esto definimos la funcion lamba a cada fila de la columna review utilizando el metodo apply. Se hace todo el proceso de tokenizacion mostrado anteriormente y se vuelve a juntar el texto con el metodo join(). El string resultante es ahora el nuevo valor para la fila en la columna 'review_es'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_es'] = df_reviews['review_es'].apply(lambda x: ' '.join([snowball_stemmer.stem(word.lower()) for word in wordpunct_tokenize(x) if (word.isalpha() and word.lower() not in stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_es'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el *CountVectorizer()* que nos da la cuenta de cada de cada palabra para cada review, lo cual podemos usar para los modelos que crearemos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la cuenta de cada palabra para cada review\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_reviews['review_es'])\n",
    "y = df_reviews['sentimiento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos**\n",
    "\n",
    "### **Multinomial Naive Bayes**\n",
    "\n",
    "Escogimos este algoritmo porque trabaja muy bien con datos discretos, lo cual lo hace especialmente bueno para una tarea de clasificacion como si el comentario es positivo o negativo. El algoritmo realiza el calculo de la probabilidad de que cada palabra de una review pertenezca a una clase (positivo o negativo) y luego utiliza estas probabilidades con el teorema de Bayes para calcular la probabilidad de que una review pertenezca a una clase. El algoritmo asume que la probabilidad de una palabra es independiente de las otras, por lo cual es llamado 'naive' Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "multinomialNB = MultinomialNB()\n",
    "multinomialNB.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_train_pred = multinomialNB.predict(X_train)\n",
    "y_pred = multinomialNB.predict(X_test)\n",
    "\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Escogimos este algoritmo porque este es capaz de manejar caracteristicas de alta dimension lo que favorece en el analisis de texto por la extension de los mismos. Ademas, maneja muy bien el overfitting. El algoritmo se basa en arboles de decision y funciona de la siguiente forma: Selecciona aleatoriamente un subconjunto de muestras; Selecciona aleatoriamente un subconjunto de caracteristicas; con las muestras y caracteristicas seleccionadas se construyen varios arboles de decision; por ultimo se selecciona el arbol de decision que obtiene mayor votacion dados sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el clasificador de random forest con 100 arboles\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "# Entrenamos el modelo\n",
    "y_train_pred_rf = rf.fit(X_train, y_train)\n",
    "# Realizamos la predicción\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred_rf, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred_rf, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred_rf, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_rf_display = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_rf_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística\n",
    "Escogimos este algoritmo porque la regresión logística es particularmente adecuada para problemas de clasificación binaria, es decir, cuando solo hay dos categorías posibles (en este caso, positivo o negativo). En estos casos, la regresión logística puede ser más eficiente y precisa que otros algoritmos. Los coeficientes del modelo pueden utilizarse para determinar qué palabras o características tienen más influencia en la clasificación de los comentarios como positivos o negativos. Además puede escalar bien con grandes conjuntos de datos y se puede mejorar mediante técnicas avanzadas de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crear un modelo de regresión logística y ajustarlo a los datos de entrenamiento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en los datos de prueba y calcular la precisión\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Precisión de la regresión logística:\", accuracy)\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred_lr, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred_lr, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred_lr, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_lr_display = ConfusionMatrixDisplay(confusion_matrix=cm_lr)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_lr_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresion logistica con mas iteraciones\n",
    "Dado que se obtuvo un warning que decia que no se alcanzo la convergencia se sugeria aumentar el numero maximo de iteraciones, el que esta por defecto es 100 por lo tanto se decide establecer un valor mayor como 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo de regresión logística y ajustarlo a los datos de entrenamiento\n",
    "lr_1 = LogisticRegression(max_iter=1000)\n",
    "lr_1.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en los datos de prueba y calcular la precisión\n",
    "y_pred_lr_1 = lr_1.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_lr_1)\n",
    "\n",
    "print(\"Precisión de la regresión logística:\", accuracy)\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_lr_1))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred_lr_1, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred_lr_1, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred_lr_1, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm_lr_1 = confusion_matrix(y_test, y_pred_lr_1)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_lr_1_display = ConfusionMatrixDisplay(confusion_matrix=cm_lr_1)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_lr_1_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresion logistica con otro algoritmo de optimizacion\n",
    "Dado que aumentar el numero maximo de iteraciones no mejoro el modelo se decidio probar con otro algoritmo de optimizacion. El algoritmo utilizado por defecto es 'lbfgs' (este utiliza el método de optimización de cuasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS)) para optimizar los parámetros del modelo). Se decide intentar con saga que utiliza una versión mejorada del método de promedio de gradientes estocásticos que admite regularización elástica y regularización L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo de regresión logística y ajustarlo a los datos de entrenamiento\n",
    "lr_2 = LogisticRegression(max_iter=1000,solver='saga')\n",
    "lr_2.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en los datos de prueba y calcular la precisión\n",
    "y_pred_lr_2 = lr_2.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_lr_2)\n",
    "\n",
    "print(\"Precisión de la regresión logística:\", accuracy)\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_lr_2))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred_lr_2, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred_lr_2, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred_lr_2, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm_lr_2 = confusion_matrix(y_test, y_pred_lr_2)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_lr_2_display = ConfusionMatrixDisplay(confusion_matrix=cm_lr_2)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_lr_2_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo de regresión logística y ajustarlo a los datos de entrenamiento\n",
    "lr_3= LogisticRegression(max_iter=1000,solver='newton-cg')\n",
    "lr_3.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en los datos de prueba y calcular la precisión\n",
    "y_pred_lr_3 = lr_3.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_lr_3)\n",
    "\n",
    "print(\"Precisión de la regresión logística:\", accuracy)\n",
    "# Metricas de evaluacion\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred_lr_3))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred_lr_3, pos_label=1)))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test, y_pred_lr_3, pos_label=1)))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test, y_pred_lr_3, pos_label=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusion\n",
    "cm_lr_3 = confusion_matrix(y_test, y_pred_lr_3)\n",
    "\n",
    "# Crear una instancia de ConfusionMatrixDisplay con la matriz de confusión\n",
    "cm_lr_3_display = ConfusionMatrixDisplay(confusion_matrix=cm_lr_3)\n",
    "\n",
    "# Mostrar la matriz de confusión utilizando la función plot() de ConfusionMatrixDisplay\n",
    "cm_lr_3_display.plot(cmap='Reds')\n",
    "\n",
    "# Añadir etiquetas y títulos\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI fine tuned classification\n",
    "Se intenta utilizar un algoritmo usando tecnologia de punta, no se logra ka total implementacion debido a que tarda mucgo tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_reviews.copy()\n",
    "df_copy['sentimiento'].replace({0 : 'negativo', 1 : 'positivo'}, inplace=True)\n",
    "df_copy.rename(columns={'review_es': 'prompt', 'sentimiento': 'completion'}, inplace=True)\n",
    "df_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_json(\"movies.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai\n",
    "!openai tools fine_tunes.prepare_data -f movies.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"movies_prepared_train.jsonl\" -m ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "Se prueba el modelo con el conjunto de datos sin clasificar\n",
    "Primero se preparan estos datos, luego se carga el modelo, se realizan las predicciones y se evaluan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_new = pd.read_csv('./data/MovieReviewsPruebas.csv')\n",
    "# Muestra el numero de filas y columnas del dataframe\n",
    "print(\"tamano set pruebas: \",df_reviews_new.shape)\n",
    "print(df_reviews.head(5))\n",
    "# Muestra el tipo de datos de cada columna\n",
    "print(df_reviews_new.dtypes)\n",
    "# Muestra la cantidad de valores nulos para la columna review_es\n",
    "print(\"valores nulos de review: \", df_reviews_new['review_es'].isnull().sum())\n",
    "print(\"valores duplicados de review: \", df_reviews['review_es'].duplicated().sum())\n",
    "\n",
    "# Primero creamos el diccionario de los idiomas en los que estan los comentarios\n",
    "langs2 = {}\n",
    "# Luego iteramos sobre cada comentario y determinamos su idioma\n",
    "for index, row in df_reviews_new.iterrows():\n",
    "    if detect(row['review_es']) not in langs2:\n",
    "        langs2[detect(row['review_es'])] = 1\n",
    "    else:\n",
    "        langs2[detect(row['review_es'])] += 1\n",
    "\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar repetidos\n",
    "df_reviews_new['review_es'].drop_duplicates(inplace=True)\n",
    "#Eliminar comentarios en ingles: Iteramos en cada una de las filas del dataframe\n",
    "for index, row in df_reviews_new.iterrows():\n",
    "    # Si el valor de la fila es en la elimina\n",
    "    if detect(row['review_es']) != 'es':\n",
    "        df_reviews_new.drop(index, inplace=True)\n",
    "\n",
    "# Seteamos las stopwords en español\n",
    "df_reviews_new['review_es'] = df_reviews_new['review_es'].apply(lambda x: ' '.join([snowball_stemmer.stem(word.lower()) for word in wordpunct_tokenize(x) if (word.isalpha() and word.lower() not in stop_words)]))\n",
    "df_reviews_new['review_es'].head(5)\n",
    "\n",
    "# Usamos el CountVectorizer() que nos da la cuenta de cada de cada palabra para cada review, lo cual podemos usar para los modelos que crearemos a continuación.\n",
    "# Extraer la cuenta de cada palabra para cada review\n",
    "X_new = vectorizer.transform(df_reviews_new['review_es'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred_lr_3_new = lr_3.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportación del modelo\n",
    "Ahora realizaremos los pasos correspondientes a la exportación del mejor modelo para desplegarlo al púbico. Por lo que usaremos pipelines para completar esta tarea. De esta forma, primero debemos exportar el mejor modelo. Dados los resultados obtenidos, nos dimos cuenta que la diferencia entre la regresión logística y Random forest (los cuales obtenían sultados similares) se daba en la clasificación de falsos positivos y falsos negativos, por lo que consideramos usar la regresión logística como mejor modelo ya que para el negocio es mejor dejar una película no tan querida por el público a sacar del catálogo una película que si gusta.\n",
    "Ahora, el pipeline debe ser de la siguiente forma:\n",
    "- Eliminar los comentarios que están en ingles.\n",
    "- Eliminar los duplicados\n",
    "- Eliminar nulos\n",
    "- Cambiar el string de la columna de \"Sentimiento\" de positivo y negativo por 0 y 1\n",
    "- Hacemos la tokenización de las palabras de los reviews\n",
    "- Vectorizamos\n",
    "- Aplicamos el modelo\n",
    "Una vez se haya hecho el pipeline, entrenaremos nuevamente el modelo y lo exportaremos usando la librería pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos los imports necesarios para que funcione el modelo\n",
    "import joblib as jb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Funcion para eliminar los comentarios que no esten en es\n",
    "def drop_lang(x):\n",
    "    for index, row in x.iterrows():\n",
    "        if detect(row['review_es']) != 'es':\n",
    "            x.drop(index, inplace=True)\n",
    "    return x\n",
    "\n",
    "# Funcion para cambiar los valores de la columna sentimiento\n",
    "def change_sentiment(x):\n",
    "    x['sentimiento'].replace({'negativo' : 0, 'positivo' : 1}, inplace=True)\n",
    "    return x\n",
    "\n",
    "# Funcion para eliminar los duplicados y los nulos\n",
    "def dropper(x):\n",
    "    x = x.drop([\"Unnamed: 0\"], axis=1) # Assign the modified DataFrame to x\n",
    "    x['review_es'].drop_duplicates(inplace=True)\n",
    "    x['review_es'].dropna(inplace=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definimos las funciones que nos ayudaran en la transformación de los comentarios recibidos, procedemos a crear el pipeline del modelo, entrenarlo y probarlo para posteriormente exportarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = jb.load('./data/count_vectorizer.pkl')\n",
    "preprocessing = Pipeline([\n",
    "    ('drop_lang', FunctionTransformer(drop_lang)),\n",
    "    ('dropper', FunctionTransformer(dropper)),\n",
    "    ('change_sentiment', FunctionTransformer(change_sentiment)),\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('clean_text', cleanText()),\n",
    "    ('Vectorizer', vectorizer),\n",
    "    ('Model', LogisticRegression(max_iter=1000,solver='newton-cg'))\n",
    "])\n",
    "\n",
    "# Definimos las variables que usaremos para entrenar y probar el modelo, estas deben ser los datos sin preprocesar\n",
    "df_reviews_model = pd.read_csv('./data/MovieReviews.csv')\n",
    "\n",
    "# Limpiamos los datos\n",
    "df_reviews_model = preprocessing.fit_transform(df_reviews_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model, X_test_model, y_train_model, y_test_model = train_test_split(df_reviews_model[\"review_es\"], df_reviews_model['sentimiento'], test_size=0.3, random_state=42)\n",
    "# Entrenamos el modelo\n",
    "modelo_1 = model.fit(X_train_model, y_train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8272033310201249\n"
     ]
    }
   ],
   "source": [
    "# Ahora prombamos el modelo\n",
    "y_model_pred = model.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test_model, y_model_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior pudimos comprobar que el el pipeline funiciona correctamente por lo que ahora realizaremos el entrenamiento con todos los datos limpios. y exportaremos el modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Guardamos el modelo\n",
    "modelo_final = model.fit(df_reviews_model[\"review_es\"], df_reviews_model['sentimiento'])\n",
    "# pickle.dump(modelo_final, open('./data/prediccion_reviews.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617     Me sorprendió las mejoras realizadas en una pe...\n",
      "3509    Esta película sigue un guión gráfico muy simil...\n",
      "2816    Un molesto y sin talento, el creador documenta...\n",
      "2578    Finalmente he visto el incubus después de espe...\n",
      "8       Asesina sexy Tiffany (Jennifer Tilly) todavía ...\n",
      "Name: review_es, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_test_model.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo debemos exportar el modelo. Debido a fallos con FastAPI el modelo se debe exportar desde un archivo de python aparte, el cual se puede encontrar en la carpeta de API bajo el nombre de model_pipeline.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
